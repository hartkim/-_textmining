{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 함수를 작성하여 모듈화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 함수 모듈화\n",
    "import hanja\n",
    "from hanja import hangul\n",
    "import re\n",
    "\n",
    "def clean_title(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그 제거\n",
    "    text = text.replace('\\\"', '').replace('\\'', '')  # 따옴표 제거\n",
    "    text = re.sub(r'[\\[\\]\\(\\)▶◀#$^&*[\\]{}<>/|』◇◆▲△■□=·●]', '', text)  # 특수기호 및 특수문자 제거\n",
    "    text = hanja.translate(text, 'substitution')  # 한자 변환\n",
    "    return text.strip()\n",
    "\n",
    "def clean_date(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그 제거\n",
    "    text = text.replace('\\\"', '').replace('\\'', '')  # 따옴표 제거\n",
    "    text = re.sub(r'(\\d{4}-\\d{2}-\\d{2})\\s.*', r'\\1', text)  # 시간 제거\n",
    "    return text.strip()\n",
    "\n",
    "def clean_content(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그 제거\n",
    "    text = text.replace('\\\"', '').replace('\\'', '')  # 따옴표 제거\n",
    "    text = re.sub(r'[\\[\\]\\(\\)▶◀#$^&*[\\]{}<>/|』◇◆▲△■□=·●]', '', text)  # 특수기호 및 특수문자 제거\n",
    "    text = re.sub(r'\\n+', ' ', text)    #줄바꿈 제거\n",
    "    # 본문 내용 전용 전처리\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # 이메일 주소 제거\n",
    "    return text.strip()\n",
    "\n",
    "def clean_company(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그 제거\n",
    "    text = text.replace('\\\"', '').replace('\\'', '')  # 따옴표 제거\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 함수를 불러온 이후, 뉴스 기사, 언론사, 발행 날짜를 크롤링하고 csv 형태로 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from datetime import datetime, timedelta\n",
    "from user_agent import generate_user_agent\n",
    "from urllib.parse import urljoin\n",
    "from .clean import *\n",
    "\n",
    "'''\n",
    "#가상환경 생성\n",
    "python = 3.8 버전으로 가상환경 생성\n",
    "\n",
    "#필요 라이브러리 설치\n",
    "pip install hanja\n",
    "pip install chardet\n",
    "pip install user_agent\n",
    "\n",
    "#스크래피 실행 및 파일 저장(cmd 창에 입력)\n",
    "scrapy crawl finance -o '파일명'.csv -t csv\n",
    "'''\n",
    "\n",
    "headers = {'User-Agent': generate_user_agent(os='win', device_type='desktop')}\n",
    "\n",
    "class NaverFinanceNewsSpider(scrapy.Spider):\n",
    "    name = 'finance'\n",
    "    allowed_domains = ['finance.naver.com']\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NaverFinanceNewsSpider, self).__init__(*args, **kwargs)\n",
    "        self.base_url = 'https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=0&y=0&sm=all.basic&pd=4&stDateStart={}&stDateEnd={}&page={}'\n",
    "        self.current_date = datetime(2009, 1, 1)\n",
    "        self.end_date = datetime(2021, 12, 31)\n",
    "        self.current_page = 1\n",
    "\n",
    "    def start_requests(self):\n",
    "        #크롤링 시작 url 생성\n",
    "        date_str = self.current_date.strftime('%Y-%m-%d')\n",
    "        url = self.base_url.format(date_str, date_str, self.current_page)\n",
    "        yield scrapy.Request(url=url, callback=self.parse, headers=headers)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 기사 목록이 없을 경우 다음 날짜로 넘어감\n",
    "        if not response.css('#contentarea_left > div.newsSchResult > dl > dt.articleSubject'):\n",
    "            self.current_date += timedelta(days=1)\n",
    "            #다음 날짜가 종료 날짜 이전일 경우 다시 parse함수 실행\n",
    "            if self.current_date <= self.end_date:\n",
    "                self.current_page = 1\n",
    "                date_str = self.current_date.strftime('%Y-%m-%d')\n",
    "                url = self.base_url.format(date_str, date_str, self.current_page)\n",
    "                yield scrapy.Request(url=url, callback=self.parse, headers=headers)\n",
    "            return\n",
    "\n",
    "        # 기사 목록이 있을 경우 기사 url 크롤링 진행\n",
    "        detail_urls = response.css('#contentarea_left > div.newsSchResult > dl > dd.articleSubject a::attr(href), dt.articleSubject a::attr(href)').getall()\n",
    "        for detail_url in detail_urls:\n",
    "            try:\n",
    "                absolute_url = urljoin('https://finance.naver.com', detail_url)\n",
    "                yield scrapy.Request(url=absolute_url, callback=self.parse_detail, headers=headers)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # 다음 페이지로 넘어감\n",
    "        self.current_page += 1\n",
    "        date_str = self.current_date.strftime('%Y-%m-%d')\n",
    "        next_url = self.base_url.format(date_str, date_str, self.current_page)\n",
    "        yield scrapy.Request(url=next_url, callback=self.parse, headers=headers)\n",
    "\n",
    "    #상세 뉴스 페이지 내용 크롤링(제목, 날짜, 본문, 신문사)\n",
    "    def parse_detail(self, response):\n",
    "        title = response.xpath('//*[@id=\"contentarea_left\"]/div[2]/div[1]/div[2]/h3/text()').get()\n",
    "        date = response.xpath('//*[@id=\"contentarea_left\"]/div[2]/div[1]/div[2]/div/span/text()').get()\n",
    "        company = response.xpath('//*[@id=\"contentarea_left\"]/div[2]/div[1]/div[1]/span/img/@alt').get()\n",
    "\n",
    "        #본문 p태그 유무에 따라 크롤링\n",
    "        content_texts = response.xpath('//*[@id=\"content\"]/p/text()').getall()\n",
    "        contents = content_texts if content_texts else response.xpath('//*[@id=\"content\"]/text()').getall()\n",
    "\n",
    "        cleaned_title = clean_title(title)\n",
    "        cleaned_date = clean_date(date)\n",
    "        cleaned_contents = ' '.join(clean_content(c) for c in contents)\n",
    "        cleaned_company = clean_company(company)\n",
    "\n",
    "        yield {\n",
    "            'date': cleaned_date,\n",
    "            'title': cleaned_title,\n",
    "            'company': cleaned_company,\n",
    "            'contents': cleaned_contents\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
